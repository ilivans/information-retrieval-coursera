{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DSSM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment you are going to create and train a DSSM neural network. If you need a recap, feel free to read the original paper [Learning Deep Structured Semantic Models for Web Search using Clickthrough Data](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/cikm2013_DSSM_fullversion.pdf) by Po-Sen Huang, Xiaodong He, Jianfeng Gao, Li Deng, Alex Acero, Larry Heck.\n",
    "\n",
    "The training dataset consists of query-document pairs from user sessions in [Yandex](http://yandex.com) search engine. Documents are represented by titles of those pages, where user was taken after clicking on the search results. The dataset was intentionally made small for you not to spend a lot of time on training. It was also somewhat filtered. Only english query-document pairs were left. Some adult content was removed.\n",
    "\n",
    "As in the original paper we will use trigrams to represent input text. \"Some, phrase!\" gives the following trigrams: \\[\"som\", \"ome\", \"phr\", \"hra\", \"ras\", \"ase\"\\].\n",
    "\n",
    "Some code has already been written to help you with this assignment. Put your code in places mark by comment YOUR CODE HERE. For several basic functions tests are given. Make sure that all tests pass before you move on.\n",
    "\n",
    "There are some prerequisits that you have to install for this notebook to run. We are going to need numpy, pandas, tensorflow and scipy. Install them via any method you find most convinient (pip, conda, etc).\n",
    "\n",
    "Ask on the forum, if you need help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ilivans/.virtualenvs/cmn36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Import all the libraries.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from scipy.spatial import cKDTree\n",
    "from tensorflow import keras\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define necessary constants.\n",
    "\n",
    "ALPHABET = ' ' + string.ascii_lowercase + string.digits\n",
    "ALPHABET_SIZE = len(ALPHABET)\n",
    "\n",
    "INEXISTANT_TRIGRAM = 0\n",
    "TOTAL_TRIGRAMS = ALPHABET_SIZE ** 3 + 1\n",
    "DOCUMENTS_PER_GROUP = 5\n",
    "\n",
    "# We will read our dataset chunk by chunk.\n",
    "CHUNK_SIZE = 10**4\n",
    "\n",
    "# Queries and titles have arbitrary length, but to simplify things, \n",
    "# we will truncate them if they are very long, or will pad them \n",
    "# with inexistant trigrams if the are to short.\n",
    "QUERY_PADDING_SIZE = 40\n",
    "TITLE_PADDING_SIZE = 80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Neural networks use numbers as inputs, so let us convert trigrams to numbers.\n",
    "# Take a look at tests below to get the idea of this encoding.\n",
    "# This function should not return 0, because 0 is reserved for INEXISTANT_TRIGRAM.\n",
    "char_to_index = dict(zip(ALPHABET, range(ALPHABET_SIZE)))\n",
    "\n",
    "def trigram_to_index(trigram):\n",
    "    a, b, c = trigram\n",
    "    index = 1 + char_to_index[c] + ALPHABET_SIZE * char_to_index[b] + ALPHABET_SIZE ** 2 * char_to_index[a]\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert trigram_to_index('   ') == 1\n",
    "assert trigram_to_index('  a') == 2\n",
    "assert trigram_to_index(' aa') == 39\n",
    "assert trigram_to_index('aaa') == 1408\n",
    "assert trigram_to_index('aa ') == 1407\n",
    "assert trigram_to_index('zzz') == 36583\n",
    "assert trigram_to_index('000') == 37990\n",
    "assert trigram_to_index('7f ') == 46769"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original input text may contain punctuation, different case, extra spaces. \n",
    "# Lets convert everything to lower case and filter out all characters except [a-z0-9 ].\n",
    "import re\n",
    "def filter_text(text):\n",
    "    return re.sub(' {2,}', ' ', re.sub('[^a-z0-9 ]+', '', text.lower())).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert filter_text('AAA') == 'aaa'\n",
    "assert filter_text('A   B') == 'a b'\n",
    "assert filter_text('  123  asdf    ') == '123 asdf'\n",
    "assert filter_text('  !@#$%^&*()-  +=`~,./?_  ') == ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With the help of the two functions defined above we are ready to implement \n",
    "# a function that converts arbitrary text to trigrams. \n",
    "# Note that it should return np.array.\n",
    "# Truncate/pad the number of output trigrams to padding_size with INEXISTANT_TRIGRAM.\n",
    "\n",
    "def text_to_trigram_vector(text, padding_size):\n",
    "    vec = []\n",
    "    text = ' ' + filter_text(text) + ' '\n",
    "    for i in range(min(len(text) - 2, padding_size)):\n",
    "        if text[i + 1] != ' ':\n",
    "            vec.append(trigram_to_index(text[i: i + 3]))\n",
    "    return np.asarray(vec + [INEXISTANT_TRIGRAM] * (padding_size - len(vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert type(text_to_trigram_vector('aaa', 3)) is type(np.array([]))\n",
    "assert np.array_equal(text_to_trigram_vector('aaa', 3), np.array([39, 1408, 1407]))\n",
    "assert np.array_equal(text_to_trigram_vector('aaa', 2), np.array([39, 1408]))\n",
    "assert np.array_equal(text_to_trigram_vector('aaa', 4), np.array([39, 1408, 1407, 0]))\n",
    "assert np.array_equal(text_to_trigram_vector('aaa aaa', 7), np.array([39, 1408, 1407, 39, 1408, 1407, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df is pandas dataframe. \n",
    "# This function computes and adds to new columns to df: \n",
    "# 'query_vector' and 'title_vector' which are numerical \n",
    "# trigram representations of query and title columns.\n",
    "\n",
    "def add_vectors(df):\n",
    "    df['query_vector'] = df['query'].apply(partial(text_to_trigram_vector, padding_size=QUERY_PADDING_SIZE))\n",
    "    df['title_vector'] = df['title'].apply(partial(text_to_trigram_vector, padding_size=TITLE_PADDING_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the data generator function. It reads input csv file in chunks and generates\n",
    "# data for training. As in the original paper we take one query, duplicate it 5 times, then take\n",
    "# the corresponding document title, put in the first position and put \"negative\" document titles in the\n",
    "# 2nd - 5th positions. \n",
    "#\n",
    "# There are several ways to construct negatives. The first one is to obtain them \n",
    "# from user sessions, but this way is not cheap. The second one, soft negatives, is to\n",
    "# randomly choose titles of other documents. And the third, hard negatives, is to choose such\n",
    "# titles of other documents that increase the error of the current model. Here we implement the\n",
    "# second and third approaches in the chunk_size window.\n",
    "\n",
    "def data_generator(filename, batch_size, chunk_size, query_model=None, document_model=None):\n",
    "    df_reader = pd.read_csv(filename, iterator=True, chunksize=chunk_size)\n",
    "    for df in df_reader:\n",
    "        df = df.dropna()\n",
    "        add_vectors(df)\n",
    "        \n",
    "        query_groups = []\n",
    "        title_groups = []\n",
    "        targets = []\n",
    "\n",
    "        for row in df.itertuples():\n",
    "            index, query, positive_title, query_vector, positive_title_vector = row\n",
    "            negative_title_vectors = []\n",
    "            \n",
    "            good_df = df[df['query'] != query]\n",
    "            if query_model is None or document_model is None:\n",
    "                negative_title_vectors.extend(good_df['title_vector'].sample(DOCUMENTS_PER_GROUP - 1))\n",
    "            else:\n",
    "                query_emedding = predict_for_one(query_model, query_vector)\n",
    "                how_close = lambda title_vector: np.inner(query_emedding, predict_for_one(document_model, title_vector))\n",
    "                good_subsample = good_df['title_vector'].sample(batch_size * 2)\n",
    "                closest = good_subsample.apply(how_close).nlargest(DOCUMENTS_PER_GROUP - 1)\n",
    "                negative_title_vectors.extend(good_subsample[closest.index])\n",
    "            query_groups.append([query_vector] * DOCUMENTS_PER_GROUP)\n",
    "            title_groups.append([positive_title_vector] + negative_title_vectors)\n",
    "\n",
    "            probabilities = np.zeros(DOCUMENTS_PER_GROUP)\n",
    "            probabilities[0] = 1\n",
    "\n",
    "            targets.append(probabilities)\n",
    "\n",
    "            if len(query_groups) == batch_size:\n",
    "                yield [np.array(query_groups), np.array(title_groups)], np.array(targets)\n",
    "                query_groups = []\n",
    "                title_groups = []\n",
    "                targets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions.\n",
    "\n",
    "def cosine_distance(query_semantic_feature, document_semantic_feature):\n",
    "    distance = tf.losses.cosine_distance(query_semantic_feature, document_semantic_feature, axis=-1, reduction=tf.losses.Reduction.NONE)\n",
    "    distance = tf.reshape(distance, [-1, DOCUMENTS_PER_BATCH])\n",
    "    return distance\n",
    "\n",
    "def sum_over_axis(axis):\n",
    "    return lambda x: tf.reduce_sum(x, axis=axis)\n",
    "\n",
    "def mean_over_axis(axis):\n",
    "    return lambda x: tf.reduce_mean(x, axis=axis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to define our DSSM model. We will use [tensorflow](https://www.tensorflow.org) for that purpose and its [keras API](https://www.tensorflow.org/guide/keras). Our model will consist of three submodels that share weights: full_model, query_model, title_model. Full model is the whole DSSM model as is. Query model is its part that calculates query embedings. Title model calculates title embedings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dssm_models(embedding_size, query_layers_params, title_layers_params):\n",
    "    # Construct inputs.\n",
    "    query_input = keras.Input(shape=(DOCUMENTS_PER_GROUP, QUERY_PADDING_SIZE,), name='queries_input')\n",
    "    title_input = keras.Input(shape=(DOCUMENTS_PER_GROUP, TITLE_PADDING_SIZE), name='titles_input')\n",
    "\n",
    "    # Define functional layer for common embeddings.\n",
    "    embedding_layer = keras.layers.Embedding(TOTAL_TRIGRAMS, embedding_size, name='common_embedding')\n",
    "    \n",
    "    # Define functionl dense layers for queries.\n",
    "    query_layers = []\n",
    "    for i, units in enumerate(query_layers_params[:-1]):\n",
    "        query_layers.append(keras.layers.Dense(units, activation='tanh', name='query_projection_{}'.format(i)))\n",
    "    query_layers.append(keras.layers.Dense(units, activation='tanh', name='query_semantic'))\n",
    "    \n",
    "    # Define functional dense layers for titles.\n",
    "    title_layers = []\n",
    "    for i, units in enumerate(title_layers_params[:-1]):\n",
    "        query_layers.append(keras.layers.Dense(units, activation='tanh', name='title_projection_{}'.format(i)))\n",
    "    title_layers.append(keras.layers.Dense(units, activation='tanh', name='title_semantic'))\n",
    "    \n",
    "    # Construct neural network for queries.                 \n",
    "    query_word_embeddings = embedding_layer(query_input)\n",
    "    query_embedding = keras.layers.Lambda(mean_over_axis(-2), name='query_word_embeddings_sum')(query_word_embeddings)\n",
    "    query_inner_layer = query_embedding\n",
    "    for layer in query_layers:\n",
    "        query_inner_layer = layer(query_inner_layer)\n",
    "    query_semantic_feature = keras.layers.Lambda(lambda x: tf.nn.l2_normalize(x, axis=-1), name='normalized_query_semantic_feature')(query_inner_layer)\n",
    "\n",
    "    # Construct neural network for titles.\n",
    "    title_word_embeddings = embedding_layer(title_input)\n",
    "    title_embedding = keras.layers.Lambda(mean_over_axis(-2), name='title_word_embeddings_sum')(title_word_embeddings)\n",
    "    title_inner_layer = title_embedding\n",
    "    for layer in title_layers:\n",
    "        title_inner_layer = layer(title_inner_layer)\n",
    "    title_semantic_feature = keras.layers.Lambda(lambda x: tf.nn.l2_normalize(x, axis=-1), name='normalized_title_semantic_feature')(title_inner_layer)\n",
    "\n",
    "    # Combine results to evaluate relevance.\n",
    "    relevance = keras.layers.Multiply(name='relevance_mult')([query_semantic_feature, title_semantic_feature])\n",
    "    relevance =  keras.layers.Lambda(sum_over_axis(-1), name='relevance_sum')(relevance)\n",
    "\n",
    "    # Calculate posterior probabilities with softmax. Find the appropriate.\n",
    "    posterior_probability = keras.layers.Softmax()(relevance)\n",
    "    \n",
    "    # Define seperate models with common weights.\n",
    "    full_model = keras.Model(inputs=[query_input, title_input], outputs=posterior_probability)\n",
    "    query_model = keras.Model(inputs=query_input, outputs=query_semantic_feature)\n",
    "    title_model = keras.Model(inputs=title_input, outputs=title_semantic_feature)\n",
    "    \n",
    "    full_model.compile(optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "                       loss='categorical_crossentropy',\n",
    "                       metrics=['accuracy'])\n",
    "    query_model.compile(optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "                        loss='categorical_crossentropy',\n",
    "                        metrics=['accuracy'])\n",
    "    title_model.compile(optimizer=keras.optimizers.Adam(lr=0.001),\n",
    "                           loss='categorical_crossentropy',\n",
    "                           metrics=['accuracy'])\n",
    "    \n",
    "    return full_model, query_model, title_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After we trained our models with groups of 5 query-document pairs, \n",
    "# we are going to use this model to predict pobability of one pair.\n",
    "\n",
    "def predict_for_one(model, inp):\n",
    "    return model.predict(inp[np.newaxis, np.newaxis, :].repeat(DOCUMENTS_PER_GROUP, axis=1))[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us define a class, that will use the trained models to index documents\n",
    "# and rank them for a given search query.\n",
    "# For every document we will calculate its title embedding and store it in KDTree,\n",
    "# a data structure that supports fast search of top_k nearest neighbors of an arbitrary vector.\n",
    "# During search a query is also converted to its embedding and then we find top_k documents with closest embeddings.\n",
    "\n",
    "class Searcher:\n",
    "    def __init__(self, query_model, document_model):\n",
    "        self.query_model = query_model\n",
    "        self.document_model = document_model\n",
    "    \n",
    "    def index(self, titles):\n",
    "        self.titles = titles\n",
    "        # Map titles to trigrams with correct padding.\n",
    "        title_vectors = keras.backend.variable(np.asarray(\n",
    "            [text_to_trigram_vector(t, TITLE_PADDING_SIZE) for t in titles]\n",
    "        ))\n",
    "        # Use document_model to predict embedings for each title vector.\n",
    "        title_embeddings = keras.backend.eval(self.document_model(title_vectors))\n",
    "        # Store embedings in KDTree for fast search.\n",
    "        self.titles_tree = cKDTree(np.vstack(title_embeddings))\n",
    "        \n",
    "    def search(self, query, top=5):\n",
    "        # Map query to trigrams with correct padding.\n",
    "        query_vector = keras.backend.variable(np.asarray(\n",
    "            [text_to_trigram_vector(query, QUERY_PADDING_SIZE)]\n",
    "        ))\n",
    "        # Use query_model to predict embeding for the query vector.\n",
    "        query_embedding = keras.backend.eval(self.query_model(query_vector))[0]\n",
    "        # Run top_k query. \n",
    "        title_indices = self.titles_tree.query(query_embedding, top)[1]\n",
    "        return title_indices\n",
    "    \n",
    "    def search_titles(self, query, top=5):\n",
    "        # Same as search, but returns titles, not thier indices in the index.\n",
    "        return self.titles[self.search(query, top)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need a function to evaluate the quality of our searcher.\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "def evalute_searcher_quality(searcher, df, top=5):\n",
    "    hits = 0\n",
    "    total = 0\n",
    "    for row in df.itertuples():\n",
    "        print(total)\n",
    "        index, query, title = row\n",
    "        if title in searcher.search_titles(query, top):\n",
    "            hits += 1\n",
    "        total += 1\n",
    "    return hits / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will train our model on the training data and then evaluate it on the validation dataset.\n",
    "\n",
    "df_val = pd.read_csv('en-query-title-unique-validation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "queries_input (InputLayer)      (None, 5, 40)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "common_embedding (Embedding)    multiple             6483712     queries_input[0][0]              \n",
      "                                                                 titles_input[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "query_word_embeddings_sum (Lamb (None, 5, 128)       0           common_embedding[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "titles_input (InputLayer)       (None, 5, 80)        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "query_projection_0 (Dense)      (None, 5, 128)       16512       query_word_embeddings_sum[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "query_semantic (Dense)          (None, 5, 128)       16512       query_projection_0[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "title_word_embeddings_sum (Lamb (None, 5, 128)       0           common_embedding[1][0]           \n",
      "__________________________________________________________________________________________________\n",
      "title_projection_0 (Dense)      (None, 5, 128)       16512       query_semantic[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "title_semantic (Dense)          (None, 5, 128)       16512       title_word_embeddings_sum[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "normalized_query_semantic_featu (None, 5, 128)       0           title_projection_0[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "normalized_title_semantic_featu (None, 5, 128)       0           title_semantic[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "relevance_mult (Multiply)       (None, 5, 128)       0           normalized_query_semantic_feature\n",
      "                                                                 normalized_title_semantic_feature\n",
      "__________________________________________________________________________________________________\n",
      "relevance_sum (Lambda)          (None, 5)            0           relevance_mult[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "softmax_6 (Softmax)             (None, 5)            0           relevance_sum[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 6,549,760\n",
      "Trainable params: 6,549,760\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "full_model, query_model, document_model = dssm_models(embedding_size=128, \n",
    "                                                      query_layers_params=(128, 64), \n",
    "                                                      title_layers_params=(128, 64))\n",
    "print(full_model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make sure that the previous cell outputs something like this:**\n",
    "```__________________________________________________________________________________________________\n",
    "Layer (type)                    Output Shape         Param #     Connected to                     \n",
    "==================================================================================================\n",
    "queries_input (InputLayer)      (None, 5, 40)        0                                            \n",
    "__________________________________________________________________________________________________\n",
    "common_embedding (Embedding)    multiple             6483712     queries_input[0][0]              \n",
    "                                                                 titles_input[0][0]               \n",
    "__________________________________________________________________________________________________\n",
    "query_word_embeddings_sum (Lamb (None, 5, 128)       0           common_embedding[0][0]           \n",
    "__________________________________________________________________________________________________\n",
    "titles_input (InputLayer)       (None, 5, 80)        0                                            \n",
    "__________________________________________________________________________________________________\n",
    "query_projection_0 (Dense)      (None, 5, 128)       16512       query_word_embeddings_sum[0][0]  \n",
    "__________________________________________________________________________________________________\n",
    "query_semantic (Dense)          (None, 5, 128)       16512       query_projection_0[0][0]         \n",
    "__________________________________________________________________________________________________\n",
    "title_word_embeddings_sum (Lamb (None, 5, 128)       0           common_embedding[1][0]           \n",
    "__________________________________________________________________________________________________\n",
    "title_projection_0 (Dense)      (None, 5, 128)       16512       query_semantic[0][0]             \n",
    "__________________________________________________________________________________________________\n",
    "title_semantic (Dense)          (None, 5, 128)       16512       title_word_embeddings_sum[0][0]  \n",
    "__________________________________________________________________________________________________\n",
    "normalized_query_semantic_featu (None, 5, 128)       0           title_projection_0[0][0]         \n",
    "__________________________________________________________________________________________________\n",
    "normalized_title_semantic_featu (None, 5, 128)       0           title_semantic[0][0]             \n",
    "__________________________________________________________________________________________________\n",
    "relevance_mult (Multiply)       (None, 5, 128)       0           normalized_query_semantic_feature\n",
    "                                                                 normalized_title_semantic_feature\n",
    "__________________________________________________________________________________________________\n",
    "relevance_sum (Lambda)          (None, 5)            0           relevance_mult[0][0]             \n",
    "__________________________________________________________________________________________________\n",
    "softmax (Activation)            (None, 5)            0           relevance_sum[0][0]              \n",
    "==================================================================================================\n",
    "Total params: 6,549,760\n",
    "Trainable params: 6,549,760\n",
    "Non-trainable params: 0\n",
    "__________________________________________________________________________________________________\n",
    "None```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To speed you up we provide you with a pretrained model. \n",
    "# It was trained on a larger dataset for a couple of days. \n",
    "# First on soft negatives, then on hard negatives.\n",
    "# Then weights of last layers where reset.\n",
    "\n",
    "full_model.load_weights('full_model_trained_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets try to index validation dataset with our Searcher.\n",
    "\n",
    "searcher = Searcher(query_model, document_model)\n",
    "searcher.index(df_val['title'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['brandon colby wikipedia', 'using bibtex', 'symfony certification',\n",
       "       'tpv compound srl', 'turbulence'], dtype=object)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform sanity check. Because some weights were reset, \n",
    "# the results should be random for now, thats ok to see something inadequate like\n",
    "# 'brandon colby wikipedia', 'using bibtex', 'symfony certification',\n",
    "# 'tpv compound srl', 'turbulence' or anything else not connected to python.\n",
    "\n",
    "searcher.search_titles('python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 2)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-116-f6cf1b929eb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtop\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'top{}: {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalute_searcher_quality\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msearcher\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-115-ea546294cd0f>\u001b[0m in \u001b[0;36mevalute_searcher_quality\u001b[0;34m(searcher, df, top)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mtitle\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msearcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_titles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0mhits\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mtotal\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-104-08e4be275128>\u001b[0m in \u001b[0;36msearch_titles\u001b[0;34m(self, query, top)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msearch_titles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Same as search, but returns titles, not thier indices in the index.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-104-08e4be275128>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, query, top)\u001b[0m\n\u001b[1;32m     27\u001b[0m         ))\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Use query_model to predict embeding for the query vector.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mquery_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_vector\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m# Run top_k query.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mtitle_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitles_tree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cmn36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36meval\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1035\u001b[0m   \u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \"\"\"\n\u001b[0;32m-> 1037\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mget_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_dense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1038\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1039\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cmn36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_value\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2796\u001b[0m   \u001b[0;32melif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2797\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Cannot get value inside Tensorflow graph function.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2798\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2799\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2800\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cmn36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mget_session\u001b[0;34m()\u001b[0m\n\u001b[1;32m    480\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m       \u001b[0m_initialize_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cmn36/lib/python3.6/site-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[0;34m(session)\u001b[0m\n\u001b[1;32m    763\u001b[0m       \u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_keras_initialized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muninitialized_vars\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m       \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariables_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables_initializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muninitialized_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cmn36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    927\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 929\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    930\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cmn36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1150\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1152\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1153\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1154\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cmn36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1327\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1328\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1329\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1330\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cmn36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1332\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1333\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1334\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1335\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1336\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/cmn36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1315\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m       \u001b[0;31m# Ensure any changes to the graph are reflected in the runtime.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1317\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1318\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m   1319\u001b[0m           options, feed_dict, fetch_list, target_list, run_metadata)\n",
      "\u001b[0;32m~/.virtualenvs/cmn36/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_extend_graph\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1350\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1351\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session_run_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1352\u001b[0;31m       \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExtendSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m   \u001b[0;31m# The threshold to run garbage collection to delete dead tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Evaluate the pretrained model. Results are going to be very bad, mostly zeros, but we will fix it later on.\n",
    "\n",
    "for top in (1, 5, 10):\n",
    "    print('top{}: {}'.format(top, evalute_searcher_quality(searcher, df_val, top)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a generator for soft negatives.\n",
    "\n",
    "random_negatives_generator = data_generator('en-query-title-unique-train-short.csv', batch_size=512, chunk_size=10**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And start the training process. It should take about 5-15 minutes.\n",
    "\n",
    "full_model.fit_generator(random_negatives_generator, steps_per_epoch=100, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now repeat the indexing of the validation dataset. \n",
    "\n",
    "searcher = Searcher(query_model, document_model)\n",
    "searcher.index(df_val['title'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This time sanity check should pass. Make sure that python is mentioned the titles.\n",
    "\n",
    "searcher.search_titles('python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your goal is to achieve over 0.70 top5 on the evaluation dataset, so make sure that validation dataset\n",
    "# provides you with >0.60 top1, >0.70 top5, >0.75 top10.\n",
    "\n",
    "for top in (1, 5, 10):\n",
    "    print('top{}: {}'.format(top, evalute_searcher_quality(searcher, df_val, top)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit for evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To score your achivement you have to send the results for the test dataset via the submit page on coursera."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the test data.\n",
    "\n",
    "df_test = pd.read_csv('en-query-title-unique-test-student.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index it.\n",
    "\n",
    "searcher = Searcher(query_model, document_model)\n",
    "searcher.index(df_test['title'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the submission dataframe.\n",
    "\n",
    "submission = df_test['query'].apply(lambda q: pd.Series(searcher.search(q)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the submission in the current directory.\n",
    "\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
